# SuperClaude CI Pipeline
# Fail-fast quality gates with matrix testing across Python 3.8-3.12

name: CI

on:
  push:
    branches: [main, master, develop]
  pull_request:
    branches: ['**']

permissions:
  contents: read

jobs:
  # ============================================
  # Quality Gate - Fast fail on style/lint issues
  # ============================================
  quality:
    name: Quality Gate
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install quality tools
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy types-PyYAML types-requests types-aiofiles pyyaml

      - name: Ruff lint check
        run: ruff check . --output-format=github

      - name: Ruff format check
        run: ruff format --check .

      - name: MyPy type check (non-blocking)
        run: mypy SuperClaude --ignore-missing-imports || echo "::warning::MyPy found type issues"
        continue-on-error: true

      - name: Validate agent frontmatter
        run: python scripts/validate_agents.py --strict

      - name: Validate agent JSON Schema
        run: |
          pip install jsonschema -q
          python scripts/validate_schema.py

  # ============================================
  # Claude Code Review - Dual Provider (Bedrock + Anthropic)
  # ============================================
  claude-review:
    name: Claude Code Review
    runs-on: ubuntu-latest
    timeout-minutes: 15
    # Only run on PRs, skip dependabot
    if: github.event_name == 'pull_request' && github.actor != 'dependabot[bot]'
    needs: [quality]
    continue-on-error: true

    permissions:
      contents: read
      pull-requests: write
      issues: read

    env:
      BEDROCK_CONFIGURED: ${{ secrets.AWS_BEARER_TOKEN_BEDROCK != '' }}
      ANTHROPIC_KEY_SET: ${{ secrets.ANTHROPIC_API_KEY != '' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Validate provider configuration
        id: validate_providers
        run: |
          if [[ "${{ env.BEDROCK_CONFIGURED }}" != "true" ]] && [[ "${{ env.ANTHROPIC_KEY_SET }}" != "true" ]]; then
            echo "::warning::No Claude API provider configured (AWS Bedrock or Anthropic API). Skipping review."
            echo "skip_review=true" >> $GITHUB_OUTPUT
          else
            echo "skip_review=false" >> $GITHUB_OUTPUT
            echo "Provider check passed:"
            echo "  - Bedrock: ${{ env.BEDROCK_CONFIGURED }}"
            echo "  - Anthropic: ${{ env.ANTHROPIC_KEY_SET }}"
          fi

      - name: Get PR context
        if: steps.validate_providers.outputs.skip_review != 'true'
        id: pr_context
        run: |
          echo "pr_number=${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
          echo "pr_title=${{ github.event.pull_request.title }}" >> $GITHUB_OUTPUT

          # Get files changed count
          FILES_CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | wc -l | tr -d ' ')
          echo "files_changed=$FILES_CHANGED" >> $GITHUB_OUTPUT

          # Get diff statistics
          DIFF_STATS=$(git diff --stat origin/${{ github.base_ref }}...HEAD | tail -1)
          echo "diff_stats=$DIFF_STATS" >> $GITHUB_OUTPUT

          # Get list of Python files changed (first 20)
          PYTHON_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep '\.py$' | head -20 | tr '\n' ',' | sed 's/,$//')
          echo "python_files=$PYTHON_FILES" >> $GITHUB_OUTPUT

          # Check if tests were modified
          TESTS_MODIFIED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep '^tests/' | wc -l | tr -d ' ')
          echo "tests_modified=${TESTS_MODIFIED:-0}" >> $GITHUB_OUTPUT

      - name: Claude Review (AWS Bedrock - Primary)
        if: steps.validate_providers.outputs.skip_review != 'true' && env.BEDROCK_CONFIGURED == 'true'
        id: bedrock_review
        uses: anthropics/claude-code-action@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          use_bedrock: "true"
          show_full_output: true
          max_turns: 20
          prompt: |
            # SuperClaude PR Code Review

            **Repository**: ${{ github.repository }}
            **PR Number**: #${{ steps.pr_context.outputs.pr_number }}
            **PR Title**: ${{ steps.pr_context.outputs.pr_title }}
            **Files Changed**: ${{ steps.pr_context.outputs.files_changed }}
            **Diff Stats**: ${{ steps.pr_context.outputs.diff_stats }}
            **Python Files**: ${{ steps.pr_context.outputs.python_files }}
            **Tests Modified**: ${{ steps.pr_context.outputs.tests_modified }}

            ## Review Instructions

            1. First, get the full diff using: `gh pr diff ${{ steps.pr_context.outputs.pr_number }}`

            2. Review the changes focusing on:
               - **Code Quality**: Clarity, maintainability, best practices
               - **Security**: Potential vulnerabilities, input validation
               - **Performance**: Efficiency concerns, resource usage
               - **Architecture**: Design patterns, SOLID principles

            3. Post your findings as a PR comment using `gh pr comment` with this format:

            ```markdown
            ## Claude Code Review (via AWS Bedrock)

            ### Overview
            [Brief summary of changes reviewed]

            ### Critical Issues
            [Any blocking issues that must be fixed]

            ### High Priority
            [Important improvements recommended]

            ### Medium Priority
            [Suggested improvements]

            ### Positive Observations
            [Good patterns and practices observed]

            ### Review Summary
            | Category | Rating |
            |----------|--------|
            | Security | /5 |
            | Code Quality | /5 |
            | Architecture | /5 |
            | Testing | /5 |

            *Generated by Claude Code Review (AWS Bedrock)*
            ```

          claude_args: >-
            --allowed-tools
            "Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*),Read,Glob,Grep"
        env:
          AWS_BEARER_TOKEN_BEDROCK: ${{ secrets.AWS_BEARER_TOKEN_BEDROCK }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
          CLAUDE_CODE_USE_BEDROCK: "1"

      - name: Claude Review (Anthropic API - Fallback)
        if: steps.validate_providers.outputs.skip_review != 'true' && env.ANTHROPIC_KEY_SET == 'true' && (env.BEDROCK_CONFIGURED != 'true' || steps.bedrock_review.outcome != 'success')
        id: anthropic_review
        uses: anthropics/claude-code-action@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          show_full_output: true
          max_turns: 20
          prompt: |
            # SuperClaude PR Code Review

            **Repository**: ${{ github.repository }}
            **PR Number**: #${{ steps.pr_context.outputs.pr_number }}
            **PR Title**: ${{ steps.pr_context.outputs.pr_title }}
            **Files Changed**: ${{ steps.pr_context.outputs.files_changed }}
            **Diff Stats**: ${{ steps.pr_context.outputs.diff_stats }}
            **Python Files**: ${{ steps.pr_context.outputs.python_files }}
            **Tests Modified**: ${{ steps.pr_context.outputs.tests_modified }}

            ## Review Instructions

            1. First, get the full diff using: `gh pr diff ${{ steps.pr_context.outputs.pr_number }}`

            2. Review the changes focusing on:
               - **Code Quality**: Clarity, maintainability, best practices
               - **Security**: Potential vulnerabilities, input validation
               - **Performance**: Efficiency concerns, resource usage
               - **Architecture**: Design patterns, SOLID principles

            3. Post your findings as a PR comment using `gh pr comment` with this format:

            ```markdown
            ## Claude Code Review (via Anthropic API - Fallback)

            ### Overview
            [Brief summary of changes reviewed]

            ### Critical Issues
            [Any blocking issues that must be fixed]

            ### High Priority
            [Important improvements recommended]

            ### Medium Priority
            [Suggested improvements]

            ### Positive Observations
            [Good patterns and practices observed]

            ### Review Summary
            | Category | Rating |
            |----------|--------|
            | Security | /5 |
            | Code Quality | /5 |
            | Architecture | /5 |
            | Testing | /5 |

            *Generated by Claude Code Review (Anthropic API - Fallback)*
            ```

          claude_args: >-
            --allowed-tools
            "Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*),Read,Glob,Grep"

      - name: Review status
        if: always() && steps.validate_providers.outputs.skip_review != 'true'
        run: |
          echo "## Claude Code Review Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          BEDROCK_STATUS="${{ steps.bedrock_review.outcome || 'skipped' }}"
          ANTHROPIC_STATUS="${{ steps.anthropic_review.outcome || 'skipped' }}"

          echo "| Provider | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| AWS Bedrock (Primary) | $BEDROCK_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| Anthropic API (Fallback) | $ANTHROPIC_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "$BEDROCK_STATUS" == "success" ]] || [[ "$ANTHROPIC_STATUS" == "success" ]]; then
            echo "**Result**: Review completed successfully" >> $GITHUB_STEP_SUMMARY
          elif [[ "$BEDROCK_STATUS" == "skipped" ]] && [[ "$ANTHROPIC_STATUS" == "skipped" ]]; then
            echo "**Result**: No providers configured" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Result**: Review failed (both providers)" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  # ============================================
  # Test Matrix - Python 3.8 through 3.12
  # ============================================
  test:
    name: Test (Python ${{ matrix.python-version }})
    needs: quality
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10']

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ".[test]"
          pip install pytest-cov pytest-asyncio

      - name: Run tests with coverage
        env:
          SUPERCLAUDE_OFFLINE_MODE: "1"
        run: |
          pytest tests/ -m "not slow and not integration and not archived_sdk" \
            --cov=SuperClaude \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short \
            -v

      - name: Upload coverage artifact
        if: matrix.python-version == '3.10'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage.xml
          retention-days: 7

      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.10'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ============================================
  # Coverage Gate - Core module 90% threshold
  # ============================================
  coverage-gate:
    name: Coverage Gate (core 90%)
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ".[test]"
          pip install pytest-cov pytest-asyncio

      - name: Check coverage threshold
        env:
          SUPERCLAUDE_OFFLINE_MODE: "1"
        run: |
          # Coverage threshold for core module (v6 orchestration logic)
          # SuperClaude/* modules are thin shims to archived SDK - not measured
          # setup/* is installation CLI - large codebase, tested separately
          # Only core/* is measured: loop orchestrator, quality gates, types
          # Current core/ coverage: ~95% (achieved in v6.0.0 development)
          pytest tests/core/ -m "not slow and not integration and not archived_sdk" \
            --cov=core \
            --cov-fail-under=90 \
            --tb=short \
            -q

  # ============================================
  # Build Verification - Ensure package builds
  # ============================================
  build:
    name: Build Check
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install build tools
        run: |
          python -m pip install --upgrade pip
          pip install build twine

      - name: Build package
        run: python -m build

      - name: Check package with twine
        run: twine check dist/*

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/
          retention-days: 7

  # ============================================
  # Artifact Install Test - Verify wheel installs correctly
  # ============================================
  artifact-install:
    name: Artifact Install (${{ matrix.extra }})
    needs: build
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10', '3.11', '3.12']
        extra: ['base', 'runtime', 'cli', 'full', 'test', 'dev']
        exclude:
          # Only test base/full on all Python versions; others on 3.11 only
          - python-version: '3.10'
            extra: 'runtime'
          - python-version: '3.10'
            extra: 'cli'
          - python-version: '3.10'
            extra: 'test'
          - python-version: '3.10'
            extra: 'dev'
          - python-version: '3.12'
            extra: 'runtime'
          - python-version: '3.12'
            extra: 'cli'
          - python-version: '3.12'
            extra: 'test'
          - python-version: '3.12'
            extra: 'dev'

    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: dist
          path: dist/

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Get wheel filename
        id: wheel
        run: |
          WHEEL=$(ls dist/*.whl | head -1)
          echo "filename=$WHEEL" >> $GITHUB_OUTPUT
          echo "Found wheel: $WHEEL"

      - name: Install from wheel (base)
        if: matrix.extra == 'base'
        run: pip install ${{ steps.wheel.outputs.filename }}

      - name: Install from wheel (with extras)
        if: matrix.extra != 'base'
        run: |
          # Use PEP 508 direct URL to wheel with extras
          pip install "SuperClaude[${{ matrix.extra }}] @ file://${{ github.workspace }}/${{ steps.wheel.outputs.filename }}"

      - name: Verify installation
        run: |
          pip show SuperClaude
          pip freeze | grep -i superclaude || true

      - name: Smoke test - import SuperClaude
        run: python -c "import SuperClaude; print('SuperClaude imported successfully')"

      - name: Smoke test - import __main__ and verify main()
        run: |
          python -c "
          from SuperClaude.__main__ import main
          assert callable(main), 'main() should be callable'
          print('main() is callable')
          "

      - name: Smoke test - module execution
        run: |
          # Test that python -m SuperClaude runs without crashing
          python -m SuperClaude || true
          echo "Module execution completed (exit code allowed to be non-zero)"

      - name: Run tests (test/full extras only)
        if: matrix.extra == 'test' || matrix.extra == 'full'
        run: |
          pip install pytest pytest-cov
          # Checkout just for tests (wheel is already installed)
          git clone --depth 1 https://github.com/${{ github.repository }} repo
          cd repo
          pytest tests/core/ -v --tb=short -x

      - name: Test sdist install (full extra, Python 3.11 only)
        if: matrix.extra == 'full' && matrix.python-version == '3.11'
        run: |
          # Create a fresh venv and install from sdist (--no-binary prevents wheel cache)
          python -m venv sdist-test
          source sdist-test/bin/activate
          pip install --no-binary :all: dist/*.tar.gz
          python -c "import SuperClaude; print('sdist install successful')"

  # ============================================
  # Benchmark Smoke Test
  # ============================================
  benchmark:
    name: Benchmark Smoke
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ".[test]"

      - name: Run benchmark smoke harness
        run: python benchmarks/run_benchmarks.py --suite smoke
        continue-on-error: true

  # ============================================
  # Generated Implementation Validation
  # ============================================
  generated-validation:
    name: Generated Validation
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ".[test]"

      - name: Validate generated implementations
        run: python -m SuperClaude.Quality.generated_validator --fail-on-errors

      - name: Run generated validation tests
        env:
          PYTEST_DISABLE_PLUGIN_AUTOLOAD: "1"
        run: |
          pytest tests/quality/test_generated_validator.py \
            tests/telemetry/test_evidence_store.py \
            -v --tb=short

  # ============================================
  # CI Summary - Aggregate status
  # ============================================
  ci-status:
    name: CI Status
    if: always()
    needs: [quality, test, coverage-gate, build, artifact-install]
    runs-on: ubuntu-latest
    steps:
      - name: Check CI status
        run: |
          if [[ "${{ needs.quality.result }}" == "failure" ]] || \
             [[ "${{ needs.test.result }}" == "failure" ]] || \
             [[ "${{ needs.coverage-gate.result }}" == "failure" ]] || \
             [[ "${{ needs.build.result }}" == "failure" ]] || \
             [[ "${{ needs.artifact-install.result }}" == "failure" ]]; then
            echo "CI failed"
            exit 1
          fi
          echo "CI passed"
