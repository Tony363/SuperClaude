The benchmarks clearly demonstrate the performance difference. Let me provide a comprehensive summary.

---

## Performance Analysis Summary

### Original Code Problems

```python
def find_duplicates(lst):
    duplicates = []
    for i in range(len(lst)):
        for j in range(i+1, len(lst)):
            if lst[i] == lst[j] and lst[i] not in duplicates:  # ← O(n) check!
                duplicates.append(lst[i])
    return duplicates
```

| Problem | Impact |
|---------|--------|
| Nested loops | O(n²) comparisons |
| `in` check on list | O(n) per check → O(n³) total |
| 2000 elements | 146ms vs 0.2ms optimized (**707x slower**) |

### Recommended Optimized Implementation

**Best for most cases — Two Sets approach:**

```python
def find_duplicates(lst):
    """
    O(n) time, O(n) space - single pass through list.
    """
    seen = set()
    duplicates = set()

    for item in lst:
        if item in seen:
            duplicates.add(item)
        else:
            seen.add(item)

    return list(duplicates)
```

**If you need to preserve order of first occurrence:**

```python
def find_duplicates_ordered(lst):
    """O(n) while preserving order."""
    seen = set()
    duplicates = []
    duplicates_set = set()

    for item in lst:
        if item in seen and item not in duplicates_set:
            duplicates.append(item)
            duplicates_set.add(item)
        seen.add(item)

    return duplicates
```

**Most concise (using Counter):**

```python
from collections import Counter

def find_duplicates(lst):
    """O(n) using Counter - most Pythonic."""
    return [item for item, count in Counter(lst).items() if count > 1]
```

### Benchmark Results

| Size | Original | Optimized | Speedup |
|------|----------|-----------|---------|
| 500 | 9.2 ms | 0.05 ms | **178x** |
| 1000 | 36.8 ms | 0.12 ms | **321x** |
| 2000 | 146 ms | 0.21 ms | **707x** |
| 5000 | SKIPPED | 0.44 ms | **>1000x** |

### Key Optimization Principles Applied

1. **Replace nested loops with hash-based data structures** — Sets provide O(1) lookup vs O(n) for lists
2. **Use the right data structure** — `set()` membership testing is O(1), list is O(n)
3. **Single pass algorithms** — Process data once instead of comparing all pairs
4. **Trade space for time** — O(n) extra space enables O(n) time complexity

The full benchmark code is saved to `artifacts/perf_find_duplicates.py` for your reference and future profiling needs.