"""
Codex-focused implementation persona.

Provides a lightweight strategist that optimises for rapid diff generation by
delegating to Codex-oriented heuristics instead of the full multi-persona stack.
"""

from __future__ import annotations

import datetime as _dt
import json
from typing import Any, Dict, List, Optional

from ..heuristic_markdown import HeuristicMarkdownAgent
from ...APIClients.codex_client import CodexClient, CodexConfig, CodexUnavailable


class CodexImplementer(HeuristicMarkdownAgent):
    """Lean persona that mirrors Codex-style rapid implementation workflows."""

    STRATEGIST_TIER = True
    default_extension = "py"

    QUICK_HINT_KEYWORDS = [
        "refactor",
        "rename",
        "cleanup",
        "lint",
        "docs",
        "typo",
        "comment",
        "minor",
        "small",
        "fast follow",
        "nit",
    ]

    def __init__(self, config: Dict[str, Any]):
        defaults = {
            "name": "codex-implementer",
            "description": (
                "Generate concise, Codex-style implementation diffs with minimal orchestration "
                "overhead while preserving evidence guardrails."
            ),
            "category": "core-development",
            "tools": ["write", "multi-edit", "read"],
            "triggers": ["fast-codex", "quick"],
            "focus_areas": {
                "implementation": "Rapid iteration on localized code edits with evidence capture.",
                "guardrails": "Maintains requires_evidence while bypassing heavy persona stacks."
            },
        }
        merged = {**defaults, **config}
        super().__init__(merged)
        self._codex_client: Optional[CodexClient] = None

    def validate(self, context: Dict[str, Any]) -> bool:
        """Prefer small or well-scoped implementation tasks."""
        task = str(context.get("task", "")).lower()
        if not task:
            return False

        keywords_hit = sum(1 for token in self.QUICK_HINT_KEYWORDS if token in task)
        if keywords_hit:
            return True

        # Allow fallback for explicit fast mode invocations even without keywords.
        return "fast-codex" in task or "fast codex" in task

    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Run heuristic synthesis and annotate the result for Codex telemetry."""
        result = super().execute(context)

        metadata = result.setdefault("metadata", {})
        metadata["execution_path"] = "codex"
        metadata["offline"] = metadata.get("offline", False)

        payload = self._generate_codex_payload(context)
        if payload:
            metadata["codex"] = {
                "summary": payload.get("summary", "Generated by Codex"),
                "generated_at": _dt.datetime.utcnow().isoformat() + "Z",
                "model": payload.get("model"),
            }
            result["codex_suggestions"] = payload
            result.setdefault("notes", []).append(
                f"Codex summary: {payload.get('summary', 'No summary provided')}"
            )
        else:
            result.setdefault("warnings", []).append(
                "Codex backend unavailable; falling back to heuristic fast-codex plan."
            )

        quick_notes: List[str] = result.setdefault("notes", [])
        if "Codex strategy: prioritize minimal diff with evidence attached." not in quick_notes:
            quick_notes.append("Codex strategy: prioritize minimal diff with evidence attached.")

        return result

    # ------------------------------------------------------------------ #
    # Codex integration helpers
    # ------------------------------------------------------------------ #

    def _generate_codex_payload(self, context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Invoke Codex, returning parsed JSON when available."""

        try:
            client = self._ensure_client()
        except CodexUnavailable:
            return None

        prompts = self._build_codex_prompts(context)
        if not prompts:
            return None

        system_prompt, user_prompt = prompts
        try:
            payload = client.complete_structured(system_prompt, user_prompt)
        except CodexUnavailable:
            return None

        changes = self._normalise_changes(payload.get("changes"))
        if not changes:
            return None

        return {
            "summary": payload.get("summary", "Generated by Codex"),
            "changes": changes,
            "model": client.config.model,
        }

    def _ensure_client(self) -> CodexClient:
        if self._codex_client is None:
            self._codex_client = CodexClient(CodexConfig())
        return self._codex_client

    def _build_codex_prompts(self, context: Dict[str, Any]) -> Optional[List[str]]:
        task = str(context.get("task", "")).strip()
        if not task:
            return None

        retrieved_context = context.get("retrieved_context") or []
        snippets: List[str] = []
        for entry in retrieved_context[:3]:
            if isinstance(entry, dict):
                summary = entry.get("summary") or entry.get("content") or ""
                source = entry.get("path") or entry.get("source") or "context"
                if summary:
                    snippets.append(f"[{source}] {summary[:400]}")

        system_prompt = (
            "You are an AI pair-programmer inside the SuperClaude fast Codex flow. "
            "Return STRICT JSON with the shape {\"summary\": str, \"changes\": [" 
            "{\"path\": str, \"content\": str, \"mode\": \"replace\"|\"append\"}]}. "
            "Paths must be repository-relative and valid. Keep diffs minimal and self-contained."
        )

        parts = ["Task:", task]
        flags = context.get("flags") or []
        if flags:
            parts.append("Flags: " + ", ".join(flags))
        params = context.get("parameters") or {}
        if params:
            parts.append("Parameters: " + json.dumps(params))
        if snippets:
            parts.append("Context:")
            parts.extend(snippets)

        user_prompt = "\n".join(parts)
        return [system_prompt, user_prompt]

    def _normalise_changes(self, raw_changes: Any) -> List[Dict[str, Any]]:
        changes: List[Dict[str, Any]] = []
        if not isinstance(raw_changes, list):
            return changes

        for item in raw_changes:
            if not isinstance(item, dict):
                continue
            path = item.get("path")
            content = item.get("content")
            if not path or not isinstance(path, str) or not isinstance(content, str):
                continue
            mode = item.get("mode", "replace")
            changes.append({
                "path": path,
                "content": content,
                "mode": mode if isinstance(mode, str) else "replace",
            })
        return changes
