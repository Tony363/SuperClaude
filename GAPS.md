# Gaps Between README Claims and Codebase

- `/sc:implement` is marketed as end-to-end feature delivery with evidence (`README.md:158`), yet the executor only aggregates agent notes (`SuperClaude/Commands/executor.py:718`) and marks success once a markdown artifact exists even if no git diff is produced (`SuperClaude/Commands/executor.py:362`).
- `/sc:test --coverage --e2e` is advertised as a full coverage run (`README.md:166`), but `_run_requested_tests` always invokes `pytest -q` and drops the coverage flag (`SuperClaude/Commands/executor.py:1576`, `SuperClaude/Commands/executor.py:792`).
- The README states that `--loop` iterates until quality â‰¥70 (`README.md:166`), yet the loop uses an identity improver that never changes results (`SuperClaude/Commands/executor.py:1331`) and simply re-evaluates the same payload (`SuperClaude/Quality/quality_scorer.py:239`).
- Multi-model consensus is highlighted as a shipped capability (`README.md:24`), but the facade wires keyword-based mock executors (`SuperClaude/ModelRouter/facade.py:131`) and consensus responses are mocked (`SuperClaude/ModelRouter/consensus.py:298`) because provider clients remain stubs (`SuperClaude/APIClients/openai_client.py:67`).
- Operational commands such as `/sc:build` and `/sc:git` are described as real automations (`README.md:187`-`README.md:195`), but their handlers just echo parameters (`SuperClaude/Commands/executor.py:797`, `SuperClaude/Commands/executor.py:806`, `SuperClaude/Commands/executor.py:925`) without performing any work.
